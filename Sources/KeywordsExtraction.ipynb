{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KeywordExtraction.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOwd4SVvF4R2EuFRWtDy9sI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/huynhhoc/AI-VLU/blob/main/Sources/KeywordsExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LenwicNHzW3H"
      },
      "source": [
        "Extracting Keywords with TF-IDF and Python’s Scikit-Learn\n",
        "TF-IDF can be used for a wide range of tasks including:\n",
        "1. text classification\n",
        "2. clustering / topic-modeling\n",
        "3. search\n",
        "4. keyword extraction and a whole lot more\n",
        "Source: https://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.YRuOZ4gzbIU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOOjntp3ystd"
      },
      "source": [
        "**Dataset**\n",
        "We will be using two files, one file, stackoverflow-data-idf.json has 20,000 posts and is used to compute the Inverse Document Frequency (IDF) and another file, stackoverflow-test.json has 500 posts and we would use that as a test set for us to extract keywords from. This dataset is based on the publicly available stack overflow dump from Google’s Big Query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIx_ip7nzT_8"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEB2ASv3pSoB"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# read json into a dataframe\n",
        "df_idf=pd.read_json(\"https://raw.githubusercontent.com/kavgan/nlp-in-practice/master/tf-idf/data/stackoverflow-data-idf.json\",lines=True)\n",
        "\n",
        "# print schema\n",
        "print(\"Schema:\\n\\n\",df_idf.dtypes)\n",
        "print(\"Number of questions,columns=\",df_idf.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsaFEbVt3pzz"
      },
      "source": [
        "import re\n",
        "def pre_process(text):\n",
        "    \n",
        "    # lowercase\n",
        "    text=text.lower()\n",
        "    \n",
        "    #remove tags\n",
        "    text=re.sub(\"</?.*?>\",\" <> \",text)\n",
        "    \n",
        "    # remove special characters and digits\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "df_idf['text'] = df_idf['title'] + df_idf['body']\n",
        "df_idf['text'] = df_idf['text'].apply(lambda x:pre_process(x))\n",
        "\n",
        "#show the first 'text'\n",
        "df_idf['text'][2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPsY-dDF4f3Z"
      },
      "source": [
        "# Creating the IDF\n",
        "\n",
        "CountVectorizer to create a vocabulary and generate word counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTdK8JQzpRpD"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVcmYww15KBz"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "import base64\n",
        "import requests\n",
        "\n",
        "def get_stop_words(stop_file_path):\n",
        "    \"\"\"load stop words \"\"\"\n",
        "    stopwords = requests.get(stop_file_path)\n",
        "    #stopwords = f.readlines()\n",
        "    stop_set = set(m.strip() for m in stopwords)\n",
        "    return frozenset(stop_set)\n",
        "\n",
        "#load a set of stop words\n",
        "stopwords=get_stop_words(\"https://raw.githubusercontent.com/huynhhoc/AI-VLU/main/resources/stopwords.txt\")\n",
        "\n",
        "#get the text column \n",
        "docs=df_idf['text'].tolist()\n",
        "\n",
        "#create a vocabulary of words, \n",
        "#ignore words that appear in 85% of documents, \n",
        "#eliminate stop words\n",
        "cv=CountVectorizer(max_df=0.85,stop_words=stopwords)\n",
        "word_count_vector=cv.fit_transform(docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV4RcwwN9t7s"
      },
      "source": [
        "\n",
        "word_count_vector.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y52tkT4L96cK"
      },
      "source": [
        "Let's limit our vocabulary size to 10,000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guowMnOk97wc"
      },
      "source": [
        "cv=CountVectorizer(max_df=0.85,stop_words=stopwords,max_features=10000)\n",
        "word_count_vector=cv.fit_transform(docs)\n",
        "word_count_vector.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tISTBy7A-CLS"
      },
      "source": [
        "Now, let's look at 10 words from our vocabulary. Sweet, these are mostly programming related."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFwdsCTn-FXa"
      },
      "source": [
        "list(cv.vocabulary_.keys())[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjXm_7Br-Pja"
      },
      "source": [
        "\n",
        "We can also get the vocabulary by using get_feature_names()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI7yQnZP-QxG"
      },
      "source": [
        "list(cv.get_feature_names())[2000:2015]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8Jh4fCR-ZWM"
      },
      "source": [
        "# TfidfTransformer to Compute Inverse Document Frequency (IDF)¶"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFTfzSAp-c5a"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "tfidf_transformer.fit(word_count_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvV9JHj0-jlb"
      },
      "source": [
        "\n",
        "Let's look at some of the IDF values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqCZnATm-kBE"
      },
      "source": [
        "tfidf_transformer.idf_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7dJvs7q-kXi"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2Zl9J19-rgK"
      },
      "source": [
        "# Computing TF-IDF and Extracting Keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGF98feE_BJF"
      },
      "source": [
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
        "    \n",
        "    #use only topn items from vector\n",
        "    sorted_items = sorted_items[:topn]\n",
        "\n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "\n",
        "    for idx, score in sorted_items:\n",
        "        fname = feature_names[idx]\n",
        "        \n",
        "        #keep track of feature name and its corresponding score\n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        "\n",
        "    #create a tuples of feature,score\n",
        "    #results = zip(feature_vals,score_vals)\n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "    \n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fIIB_Gi_EEK"
      },
      "source": [
        "The next step is to compute the tf-idf value for a given document in our test set by invoking tfidf_transformer.transform(...). This generates a vector of tf-idf scores. Next, we sort the words in the vector in descending order of tf-idf values and then iterate over to extract the top-n items with the corresponding feature names, In the example below, we are extracting keywords for the first document in our test set.\n",
        "\n",
        "The sort_coo(...) method essentially sorts the values in the vector while preserving the column index. Once you have the column index then its really easy to look-up the corresponding word value as you would see in extract_topn_from_vector(...) where we do feature_vals.append(feature_names[idx])."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p07AKthn_mRq"
      },
      "source": [
        "From the keywords above, the top keywords actually make sense, it talks about eclipse, maven, integrate, war and tomcat which are all unique to this specific question. There are a couple of kewyords that could have been eliminated such as possibility and perhaps even project and you can do this by adding more common words to your stop list and you can even create your own set of stop list, very specific to your domain as described here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIKfgzTO_m97"
      },
      "source": [
        "# put the common code into several methods\n",
        "def get_keywords(idx):\n",
        "\n",
        "    #generate tf-idf for the given document\n",
        "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs_test[idx]]))\n",
        "\n",
        "    #sort the tf-idf vectors by descending order of scores\n",
        "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "    #extract only the top n; n here is 10\n",
        "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
        "    \n",
        "    return keywords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPKH9P6d_rck"
      },
      "source": [
        "Now let's look at keywords generated for a much longer question:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYWMe1iG_waE"
      },
      "source": [
        "# Generate keywords for a batch of documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZVWhJCA_yij"
      },
      "source": [
        "#generate tf-idf for all documents in your list. docs_test has 500 documents\n",
        "tf_idf_vector=tfidf_transformer.transform(cv.transform(docs_test))\n",
        "\n",
        "results=[]\n",
        "for i in range(tf_idf_vector.shape[0]):\n",
        "    \n",
        "    # get vector for a single document\n",
        "    curr_vector=tf_idf_vector[i]\n",
        "    \n",
        "    #sort the tf-idf vector by descending order of scores\n",
        "    sorted_items=sort_coo(curr_vector.tocoo())\n",
        "\n",
        "    #extract only the top n; n here is 10\n",
        "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
        "    \n",
        "    \n",
        "    results.append(keywords)\n",
        "\n",
        "df=pd.DataFrame(zip(docs,results),columns=['doc','keywords'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}